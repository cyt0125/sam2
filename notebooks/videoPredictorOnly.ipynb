{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59704ce3-f615-49c1-ba64-f92b986b1f3f",
   "metadata": {},
   "source": [
    "## test code for utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d29e852-25d2-4e04-bc92-65ddfd04dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import sam2\n",
    "from sam2.build_sam import build_sam2_video_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e360a7-88ed-4f53-a202-0a6204169e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_check():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        # use bfloat16 for the entire notebook\n",
    "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        if torch.cuda.get_device_properties(0).major >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    elif device.type == \"mps\":\n",
    "        print(\n",
    "            \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "            \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "            \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "        )\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17b55be-f049-4e6d-b597-9a0fe54eb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_predictor(model_size, device):\n",
    "    checkpoint_map = {\n",
    "        'large': 'sam2.1_hiera_large.pt',\n",
    "        'base_plus': 'sam2.1_hiera_base_plus.pt',\n",
    "        'small': 'sam2.1_hiera_small.pt',\n",
    "        'tiny': 'sam2.1_hiera_tiny.pt'\n",
    "    }\n",
    "    \n",
    "    cfg_map = {\n",
    "        'large': 'sam2.1_hiera_l.yaml',\n",
    "        'base_plus': 'sam2.1_hiera_b+.yaml',\n",
    "        'small': 'sam2.1_hiera_s.yaml',\n",
    "        'tiny': 'sam2.1_hiera_t.yaml'\n",
    "    }\n",
    "    \n",
    "    if model_size not in checkpoint_map or model_size not in cfg_map:\n",
    "        raise ValueError(\"Invalid model size. Please choose from 'large', 'base_plus', 'small', or 'tiny'.\")\n",
    "    \n",
    "    checkpoint = checkpoint_map[model_size]\n",
    "    cfg = cfg_map[model_size]\n",
    "    \n",
    "    print(f\"Checkpoint: {checkpoint}, Config: {cfg}\")\n",
    "    \n",
    "    sam2_checkpoint = f\"../checkpoints/{checkpoint}\"\n",
    "    model_cfg = cfg\n",
    "    \n",
    "    predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "    \n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f2c609-3956-4390-b5cb-715016732d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(video_dir, model_size):\n",
    "    # initialize the state for inference\n",
    "    predictor = init_predictor(model_size=model_size, device=sys_check())\n",
    "    inference_state = predictor.init_state(video_path=video_dir)\n",
    "    return predictor, inference_state\n",
    "\n",
    "def reset_state(predictor,inference_state):\n",
    "    predictor.reset_state(inference_state)\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616a36fe-6dca-4863-9b09-17a6708b15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_point(\n",
    "        predictor,\n",
    "        inference_state, \n",
    "        frame_idx, \n",
    "        obj_id, \n",
    "        points=None, \n",
    "        labels=None, \n",
    "        clear_old_points=True,\n",
    "        box=None,\n",
    "    ):\n",
    "    \n",
    "    points = np.array(points, dtype=np.float32)\n",
    "    labels = np.array(labels, np.int32)\n",
    "    prompts[obj_id] = points, labels\n",
    "    out_frame_idx, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=frame_idx,\n",
    "        obj_id=obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "        clear_old_points=clear_old_points,\n",
    "        box=box,\n",
    "    )\n",
    "  \n",
    "    return out_obj_ids, out_mask_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5fa5ed-6a31-4dad-ae75-6237584a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(\n",
    "        predictor, \n",
    "        inference_state,\n",
    "        start_frame_idx=None,\n",
    "        max_frame_num_to_track=None,\n",
    "        reverse=False,\n",
    "    ):\n",
    "    # run propagation throughout the video and collect the results in a dict\n",
    "    video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "    predict_result = predictor.propagate_in_video(\n",
    "        inference_state, \n",
    "        start_frame_idx=start_frame_idx, \n",
    "        max_frame_num_to_track=max_frame_num_to_track, \n",
    "        reverse=reverse)\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predict_result:\n",
    "        video_segments[out_frame_idx] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "        }\n",
    "    return video_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301db631-1e30-46b6-a00d-f302f62bf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video_all(\n",
    "        predictor,\n",
    "        inference_state,\n",
    "        start_frame_idx,\n",
    "        max_frame_num_to_track=None,\n",
    "        reverse=False,\n",
    "    ):\n",
    "\n",
    "    if start_frame_idx > 0:\n",
    "        # 生成两个新字典\n",
    "        video_segments_pre = predict_video(predictor, inference_state, start_frame_idx=start_frame_idx-1, reverse=True)\n",
    "        video_segments_post = predict_video(predictor, inference_state, start_frame_idx=start_frame_idx)\n",
    "\n",
    "        # 合并字典\n",
    "        combined_dict = {**video_segments_pre, **video_segments_post}\n",
    "        # 排序键并创建新字典\n",
    "        sorted_keys = sorted(combined_dict.keys())\n",
    "        sorted_dict = {key: combined_dict[key] for key in sorted_keys}\n",
    "        video_segments_all = sorted_dict\n",
    "    else:\n",
    "        video_segments_all = predict_video(predictor, inference_state, start_frame_idx)\n",
    "\n",
    "    return video_segments_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a71bc81-3ba4-48a3-8c9b-6e38bcc8f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to Apply Object Effects\n",
    "def apply_object_effect(frame, mask, effect):\n",
    "    result = frame.copy()\n",
    "\n",
    "    if effect == \"erase\":\n",
    "        # Replace object with white (erased)\n",
    "        result[mask == 255] = [255, 255, 255]  # Set object area to white\n",
    "\n",
    "    elif effect == \"gradient\":\n",
    "        # Create a horizontal gradient across the width of the mask\n",
    "        gradient = np.linspace(0, 255, frame.shape[1], dtype=np.uint8)  # Generate gradient over width\n",
    "        gradient = np.tile(gradient, (frame.shape[0], 1))  # Repeat gradient across height\n",
    "        gradient_3channel = np.dstack([gradient] * 3)  # Convert to 3-channel (R, G, B)\n",
    "\n",
    "        # Apply the gradient to the object region\n",
    "        result[mask == 255] = gradient_3channel[mask == 255]\n",
    "\n",
    "    elif effect == \"pixelate\":\n",
    "        # Pixelate the object by downscaling and then upscaling the object region\n",
    "        small = cv2.resize(result, (10, 10))  # Downscale to 10x10\n",
    "        pixelated_region = cv2.resize(small, (result.shape[1], result.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        result[mask == 255] = pixelated_region[mask == 255]\n",
    "\n",
    "    elif effect == \"overlay\":\n",
    "        # Apply a green overlay to the object\n",
    "        overlay = np.full_like(result, [0, 255, 0])  # Green overlay\n",
    "        result[mask == 255] = overlay[mask == 255]\n",
    "\n",
    "    elif effect == \"emoji\":\n",
    "        # Apply an emoji overlay to the object region (Make sure emoji.png exists)\n",
    "        emoji = cv2.resize(cv2.imread(\"C:/Users/26087/Desktop/emoji.png\"), (mask.shape[1], mask.shape[0]))\n",
    "        result[mask == 255] = emoji[mask == 255]\n",
    "\n",
    "    elif effect == \"burst\":\n",
    "        result = draw_burst(result, mask)  # Use the 2D mask\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Helper Function to Apply Background Effects\n",
    "def apply_background_effect(frame, mask, effect):\n",
    "    result = frame.copy()\n",
    "\n",
    "    # Invert the mask to get the background (where mask == 0)\n",
    "    background_mask = (mask == 0)\n",
    "\n",
    "    if effect == \"erase\":\n",
    "        # Set the background to white (erased)\n",
    "        result[background_mask] = [255, 255, 255]  # Set background to white\n",
    "\n",
    "    elif effect == \"gradient\":\n",
    "        # Create a horizontal gradient across the width of the image\n",
    "        gradient = np.linspace(0, 255, frame.shape[1], dtype=np.uint8)  # Generate gradient over width\n",
    "        gradient = np.tile(gradient, (frame.shape[0], 1))  # Repeat gradient across height\n",
    "        gradient_3channel = np.dstack([gradient] * 3)  # Convert to 3-channel (R, G, B)\n",
    "\n",
    "        # Apply the gradient to the background region\n",
    "        result[background_mask] = gradient_3channel[background_mask]\n",
    "\n",
    "    elif effect == \"pixelate\":\n",
    "        # Pixelate the background by downscaling and then upscaling the background region\n",
    "        small = cv2.resize(result, (10, 10))  # Downscale to 10x10\n",
    "        pixelated_region = cv2.resize(small, (result.shape[1], result.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        result[background_mask] = pixelated_region[background_mask]\n",
    "\n",
    "    elif effect == \"desaturate\":\n",
    "        # Desaturate the background (convert to grayscale)\n",
    "        gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "        result[background_mask] = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)[background_mask]\n",
    "\n",
    "    elif effect == \"blur\":\n",
    "        # Blur the background using Gaussian blur\n",
    "        blurred_bg = cv2.GaussianBlur(result, (21, 21), 0)\n",
    "        result[background_mask] = blurred_bg[background_mask]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Function to draw rays around the object (burst effect)\n",
    "def draw_burst(image, mask):\n",
    "    result = image.copy()\n",
    "\n",
    "    # Ensure the mask is single-channel before finding contours\n",
    "    if len(mask.shape) == 3:\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If no contours are found, return the original image\n",
    "    if len(contours) == 0:\n",
    "        return result\n",
    "\n",
    "    # Get the center of the object based on the mask contours\n",
    "    contour = contours[0]  # Assuming the largest contour is the object\n",
    "    M = cv2.moments(contour)\n",
    "    if M[\"m00\"] == 0:\n",
    "        return result\n",
    "\n",
    "    # Calculate the object center from the moments\n",
    "    center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "    center_y = int(M[\"m01\"] / M[\"m00\"])\n",
    "    center = (center_x, center_y)\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Define the number of rays and the angle step between them\n",
    "    num_rays = 360  # You can adjust this for more or fewer rays\n",
    "    angle_step = 360 / num_rays\n",
    "\n",
    "    # Draw rays from the center point outward\n",
    "    for angle in np.arange(0, 360, angle_step):\n",
    "        radian = np.deg2rad(angle)\n",
    "        end_x = int(center_x + width * np.cos(radian))\n",
    "        end_y = int(center_y + width * np.sin(radian))\n",
    "\n",
    "        # Draw the line from the center to the calculated endpoint (white color, thickness 2)\n",
    "        cv2.line(result, (center_x, center_y), (end_x, end_y), (255, 255, 255), 2)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Updated apply_masks_to_video function\n",
    "def apply_masks_to_video(video_path, video_segments_all, output_path, effect, object_effect, background_effect):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the basic information of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create a VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")  # Use mp4 encoding\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Gets the mask of the current frame\n",
    "        if frame_index in video_segments_all:\n",
    "            masks = video_segments_all[frame_index]  # Gets all the masks for the current frame\n",
    "\n",
    "            for obj_id, mask in masks.items():\n",
    "                # The shape of the mask is (1, 720, 1280) and we need to convert it to (720, 1280)\n",
    "                mask = mask[0]  # Remove the first dimension and become (720, 1280)\n",
    "\n",
    "                # Convert the Boolean mask to a uint8 type\n",
    "                mask = (mask * 255).astype(np.uint8)  # Convert True/False to 255/0\n",
    "\n",
    "                if effect:\n",
    "                    # Apply object and background effects\n",
    "                    masked_frame = apply_background_effect(frame, mask, effect=background_effect)\n",
    "                    masked_frame = apply_object_effect(masked_frame, mask, effect=object_effect)\n",
    "                else:\n",
    "                    # Create a color mask\n",
    "                    colored_mask = np.zeros((height, width, 3),\n",
    "                                            dtype=np.uint8)  # Create an image that is completely black\n",
    "                    colored_mask[mask == 255] = [0, 255, 0]  # Set the mask area to green (BGR format)\n",
    "\n",
    "                    # Applies a color mask to the current frame\n",
    "                    masked_frame = cv2.addWeighted(frame, 1, colored_mask, 0.5, 0)  # Overlay the mask onto the frame\n",
    "\n",
    "                # Write the processed frame to the output video\n",
    "                out.write(masked_frame)\n",
    "        else:\n",
    "            # If there is no mask, write directly to the original frame\n",
    "            out.write(frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    output_avi_path = output_path\n",
    "\n",
    "    base_name, ext = os.path.splitext(output_avi_path)\n",
    "\n",
    "    output_mp4_path = base_name + \".mp4\"\n",
    "\n",
    "    output_avi_path = output_avi_path.replace('\\\\', '/')\n",
    "\n",
    "    output_mp4_path = output_mp4_path.replace('\\\\', '/')\n",
    "\n",
    "    print(output_mp4_path, output_avi_path)\n",
    "\n",
    "    convert_avi_to_mp4(output_avi_path, output_mp4_path)\n",
    "\n",
    "    # Optionally, you can remove the AVI file after conversion\n",
    "    # if os.path.exists(output_avi_path):\n",
    "    #     os.remove(output_avi_path)\n",
    "\n",
    "\n",
    "def convert_avi_to_mp4(input_avi, output_mp4):\n",
    "    print(input_avi, output_mp4)\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg', '-y',  # -y to overwrite the output file if it exists\n",
    "        '-i', input_avi,  # Input AVI file\n",
    "        '-vcodec', 'libx264',  # Use H.264 for video\n",
    "        '-acodec', 'aac',  # Use AAC for audio\n",
    "        '-strict', 'experimental',  # Use experimental AAC encoder if needed\n",
    "        output_mp4  # Output MP4 file\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(ffmpeg_command)\n",
    "        # Run the FFmpeg command to convert AVI to MP4\n",
    "        subprocess.run(ffmpeg_command, check=True)\n",
    "        print(f\"Conversion to MP4 successful! Saved as {output_mp4}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during conversion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c4856-c257-42ff-b1d9-ba3b2e24f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 'tiny'  # 可选 'large', 'base_plus', 'small', 'tiny'\n",
    "video_dir = './videos/test.mp4'\n",
    "\n",
    "predictor, inference_state = init_state(video_dir, model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60a922-3aa1-47af-9291-da19a2b4e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = reset_state(predictor, inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb300d-3b11-47f9-a3ef-a7dc250d93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # hold all the clicks we add for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49cfd6b-3d3c-472d-9b8f-a8b2d7dbf88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "obj_id = 1\n",
    "points = [[350,350]]\n",
    "labels = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7a4a5-2604-4996-8cfb-292bcd9cbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_obj_ids, out_mask_logits = add_point(predictor, inference_state, frame_idx, obj_id, points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11f929-9b3c-4a9f-8ce8-743f75af6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_segments = predict_video(predictor, inference_state, start_frame_idx=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0477cb-7330-406b-8ff7-f6c279a773c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame_idx = 0\n",
    "\n",
    "video_segments_all = predict_video_all(predictor, \n",
    "        inference_state,\n",
    "        start_frame_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20bf8a2-c8a7-473b-9124-082ba0f86af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例用法\n",
    "video_path = video_dir  # 输入视频路径\n",
    "masks_dict = video_segments_all\n",
    "\n",
    "output_path = 'output_video.mp4'  # 输出视频路径\n",
    "\n",
    "apply_masks_to_video(video_path, masks_dict, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a056172-1521-42a9-b4f6-a31d30b844a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
